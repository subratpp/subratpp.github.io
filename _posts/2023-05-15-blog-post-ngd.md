---
title: 'Gradient Free Approaches to Update Weights in Deep Learning'
date: 2023-05-15
permalink: /posts/2023/05/blog-post-ngd/
tags:
  - Deep Learning
  - Gradient-Free Descent
  - Optimization
---
Coming soon!
{: .notice}

1.	Difference Target Propagation
https://arxiv.org/pdf/1412.7525.pdf

2.	The HSIC Bottleneck (Hilbert-Schmidt Independence Criterion) 
https://arxiv.org/pdf/1908.01580v1.pdf
3.	Online Alternating Minimization with Auxiliary Variables
https://arxiv.org/pdf/1806.09077.pdf
4.	Decoupled Neural Interfaces Using Synthetic Gradients
https://arxiv.org/pdf/1608.05343.pdf
5.	Accelerated Stochastic Gradient-free and Projection-free Methods
6.	On Correctness of Automatic Differentiation for Non-Differentiable Functions
7.	Direct Feedback Alignment Provides Learning in Deep Neural Networks
8.	Cubature Kalman filtering for training deep neural networks



Nelder-Mead simplex algorithm, the Powell's method, and the Hooke-Jeeves method.


Non-Differentiable Gradient Descent

1.	Smooth Approximation
2.	E-subgradient method
3.	Cutting plane method
4.	Subgradient method
5.	EA
6.	Conjugate gradient method
7.	Hessian free optimization method
8.	Quasi-Newton method
9.	GA
10.	BFGS
11.	Simulated Annealing

Notable derivative-free optimization algorithms include:
●	Bayesian optimization
●	Coordinate descent and adaptive coordinate descent
●	Cuckoo search
●	Beetle Antennae Search (BAS)
●	DONE
●	Evolution strategies, Natural evolution strategies (CMA-ES, xNES, SNES)
●	Genetic algorithms
●	MCS algorithm
●	Nelder-Mead method
●	Particle swarm optimization
●	Pattern search
●	Random search (including Luus–Jaakola)
●	Simulated annealing
●	Stochastic optimization
●	Subgradient method




References
------
